#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# --- Configuration ---
FRONTEND_PORT=8000
BACKEND_DIR="obpi_cde_backend"
FRONTEND_PID_FILE=".frontend.pid"
BACKEND_PID_FILE=".backend.pid"
BACKEND_LOG_FILE="backend.log"
FRONTEND_LOG_FILE="frontend.log"

# --- Colors ---
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${GREEN}--- OBPI OS Development Environment Setup ---${NC}"

# --- Dependency Checks ---
echo "1. Checking dependencies..."

# Ensure ~/.cargo/bin is on PATH for this session (helps when installing cargo-edit)
export PATH="$HOME/.cargo/bin:$PATH"

# Check for Rust/Cargo
if ! command -v cargo &> /dev/null; then
    echo -e "${RED}Error: Rust is not installed. Please install it from https://rust-lang.org${NC}"
    exit 1
fi
echo -e "   - Cargo: ${GREEN}OK${NC}"

# Check for cargo-edit (cargo-add / cargo-rm / cargo-upgrade)
if ! command -v cargo-add &> /dev/null; then
    echo "   - cargo-add not found. Installing cargo-edit (provides cargo-add)..."
    cargo install cargo-edit || {
        echo -e "${RED}Error: cargo-edit installation failed. Ensure build deps are installed and ~/.cargo/bin is in PATH.${NC}"
        echo "You may need to install platform build tools (e.g. Xcode command-line tools on macOS, build-essential/pkg-config on Linux)."
        exit 1
    }
fi
echo -e "   - cargo-edit: ${GREEN}OK${NC}"

# Check for Node/npm
if ! command -v npm &> /dev/null; then
    echo -e "${RED}Error: Node.js and npm are not installed. Please install them from https://nodejs.org${NC}"
    exit 1
fi
echo -e "   - npm: ${GREEN}OK${NC}"

# Check for sqlx-cli
if ! command -v sqlx &> /dev/null; then
    echo -e "${YELLOW}Warning: sqlx-cli is not installed.${NC}"
    read -p "Do you want to install it now? (cargo install sqlx-cli) [y/N] " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        cargo install sqlx-cli --features native-tls,sqlite
        if ! command -v sqlx &> /dev/null; then
            echo -e "${RED}Error: sqlx-cli installation failed. Please install it manually and ensure it's in your PATH.${NC}"
            echo "You may need to add ~/.cargo/bin to your PATH."
            exit 1
        fi
    else
        echo -e "${RED}Error: sqlx-cli is required to set up the database.${NC}"
        exit 1
    fi
fi
echo -e "   - sqlx-cli: ${GREEN}OK${NC}"


# --- Backend Generation and Setup ---
echo -e "\n2. Generating Rust Backend from scratch..."

# Clean up any old project directory
if [ -d "$BACKEND_DIR" ]; then
    echo "   - Removing existing backend directory for a clean build..."
    rm -rf "$BACKEND_DIR"
fi

# Create new backend directory and src manually to avoid cargo workspace detection
echo "   - Creating backend directory structure: $BACKEND_DIR"
mkdir -p "$BACKEND_DIR/src"
cd "$BACKEND_DIR"
echo "   - Switched to directory: $(pwd)"

# --- Write all backend files using heredocs ---
echo "   - Writing backend configuration and code..."

# Write Cargo.toml
cat > Cargo.toml << 'EOF'
[package]
name = "obpi_cde_backend"
version = "0.3.0"
edition = "2021"

[dependencies]
tokio = { version = "1", features = ["full"] }
axum = { version = "0.7", features = ["ws"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
pty-process-tokio = "0.3"
uuid = { version = "1.7", features = ["v4"] }
base64 = "0.22"
futures-util = "0.3"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
sqlx = { version = "0.7", features = ["runtime-tokio", "sqlite", "macros", "migrate", "chrono"] }
dotenvy = "0.15"
rand = "0.8"
sha2 = "0.10"
hex = "0.4"
chrono = { version = "0.4", features = ["serde"] }
anyhow = "1.0"
EOF

# Write .env
cat > .env << 'EOF'
DATABASE_URL=sqlite:obpi_os.db
STORAGE_ROOT=/tmp/cde_storage
EOF

# Write README.md
cat > README.md << 'EOF'
# OBPI OS Cloud Kernel Backend (v0.3.0 - Seamless)
This is the Rust backend for the OBPI React Desktop project. This project was automatically generated by the `start.sh` script.
EOF

# Create src directory files
cat > src/main.rs << 'EOF'
use axum::{extract::{ws::{WebSocket, WebSocketUpgrade}, State}, response::Response, routing::get, Router};
use std::net::SocketAddr;
use std::sync::Arc;
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

mod db;
mod pty_handler;
mod protocol;
mod session;
mod vfs;

use crate::db::DbPool;
use crate::session::UserSession;

#[tokio::main]
async fn main() {
    tracing_subscriber::registry()
        .with(tracing_subscriber::EnvFilter::new(
            std::env::var("RUST_LOG").unwrap_or_else(|_| "obpi_cde_backend=debug,tower_http=debug".into()),
        ))
        .with(tracing_subscriber::fmt::layer())
        .init();

    dotenvy::dotenv().expect("Failed to read .env file");

    let db_pool = db::init_db().await.expect("Failed to initialize database");
    
    let app_state = Arc::new(db_pool);

    let app = Router::new()
        .route("/ws", get(ws_handler))
        .with_state(app_state);

    let addr = SocketAddr::from(([127, 0, 0, 1], 8080));
    tracing::debug!("listening on {}", addr);

    let listener = tokio::net::TcpListener::bind(addr).await.unwrap();
    axum::serve(listener, app).await.unwrap();
}

async fn ws_handler(
    ws: WebSocketUpgrade,
    State(state): State<Arc<DbPool>>,
) -> Response {
    ws.on_upgrade(|socket| handle_socket(socket, state))
}

async fn handle_socket(socket: WebSocket, db_pool: Arc<DbPool>) {
    tracing::debug!("New WebSocket connection received.");
    UserSession::new(socket, db_pool).run().await;
}
EOF

cat > src/db.rs << 'EOF'
use crate::protocol::UserInfo;
use rand::{Rng, thread_rng};
use sha2::{Digest, Sha256};
use sqlx::{sqlite::{Sqlite, SqlitePoolOptions}, migrate::MigrateDatabase, Row, SqlitePool};
use std::env;

pub type DbPool = SqlitePool;

pub async fn init_db() -> Result<DbPool, sqlx::Error> {
    let db_url = env::var("DATABASE_URL").expect("DATABASE_URL must be set");
    
    if !Sqlite::database_exists(&db_url).await.unwrap_or(false) {
        Sqlite::create_database(&db_url).await?;
    }

    let pool = SqlitePoolOptions::new()
        .max_connections(5)
        .connect(&db_url)
        .await?;

    tracing::info!("Running database migrations...");
    sqlx::migrate!("./migrations").run(&pool).await?;
    tracing::info!("Database migrations complete.");

    setup_initial_users(&pool).await?;

    Ok(pool)
}

fn hash_password(password: &str, salt: &[u8]) -> Vec<u8> {
    let mut hasher = Sha256::new();
    hasher.update(password.as_bytes());
    hasher.update(salt);
    hasher.finalize().to_vec()
}

pub async fn verify_password(pool: &DbPool, username: &str, password: &str) -> Result<Option<UserInfo>, anyhow::Error> {
    let row = sqlx::query("SELECT id, username, role, password_hash FROM users WHERE username = ?")
        .bind(username)
        .fetch_optional(pool)
        .await?;

    if let Some(row) = row {
        let stored_hash_str: String = row.try_get("password_hash")?;
        let parts: Vec<&str> = stored_hash_str.split(':').collect();
        if parts.len() != 2 {
            return Err(anyhow::anyhow!("Invalid password hash format in DB"));
        }

        let salt = hex::decode(parts[0])?;
        let stored_hash = hex::decode(parts[1])?;
        let provided_hash = hash_password(password, &salt);

        if provided_hash == stored_hash {
            Ok(Some(UserInfo {
                id: row.try_get("id")?,
                username: row.try_get("username")?,
                role: row.try_get("role")?,
            }))
        } else {
            Ok(None)
        }
    } else {
        Ok(None)
    }
}

async fn create_user_if_not_exists(pool: &DbPool, username: &str, password: &str, role: &str) -> Result<(), sqlx::Error> {
    let user_exists: (i64,) = sqlx::query_as("SELECT COUNT(*) FROM users WHERE username = ?")
        .bind(username)
        .fetch_one(pool)
        .await?;

    if user_exists.0 == 0 {
        tracing::info!("Creating user '{}'...", username);
        let mut rng = thread_rng();
        let salt: [u8; 16] = rng.gen();
        let password_hash = hash_password(password, &salt);
        let password_hash_str = format!("{}:{}", hex::encode(salt), hex::encode(password_hash));

        let user_id = sqlx::query("INSERT INTO users (username, password_hash, role) VALUES (?, ?, ?)")
            .bind(username)
            .bind(&password_hash_str)
            .bind(role)
            .execute(pool)
            .await?
            .last_insert_rowid();
        
        sqlx::query("INSERT INTO files (owner_id, parent_id, name, node_type, original_path) VALUES (?, NULL, ?, 'dir', ?)")
            .bind(user_id)
            .bind(format!("/home/{}", username))
            .bind(format!("/home/{}", username))
            .execute(pool)
            .await?;
        
        tracing::info!("User '{}' created successfully.", username);
    }
    Ok(())
}

async fn setup_initial_users(pool: &DbPool) -> Result<(), sqlx::Error> {
    create_user_if_not_exists(pool, "guest", "password", "Admin").await?;
    create_user_if_not_exists(pool, "root", "root", "Admin").await?;
    Ok(())
}
EOF

cat > src/protocol.rs << 'EOF'
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};

pub type RequestId = String;

#[derive(Deserialize, Debug)]
pub struct ClientRequest {
    pub request_id: RequestId,
    #[serde(flatten)]
    pub payload: ClientRequestPayload,
}

#[derive(Deserialize, Debug)]
#[serde(tag = "type", content = "payload")]
#[serde(rename_all = "camelCase")]
pub enum ClientRequestPayload {
    Login { username: String, password: String },
    RunCommand { command: String },
    VfsList { path: String },
    VfsReadFile { path: String },
    VfsWriteFile { path: String, content: String },
    VfsCreateNode { path: String, node_type: String },
    VfsMoveNode { old_path: String, new_path: String },
    VfsTrashNode { path: String },
    VfsListTrash,
    VfsRestoreNode { id: i64 },
    VfsDeleteNode { id: i64 },
    VfsEmptyTrash,
}

#[derive(Serialize, Debug)]
pub struct ServerResponse {
    pub request_id: RequestId,
    #[serde(flatten)]
    pub payload: ServerResponsePayload,
}

#[derive(Serialize, Debug)]
#[serde(tag = "type", content = "payload")]
#[serde(rename_all = "camelCase")]
pub enum ServerResponsePayload {
    LoginSuccess { user: UserInfo },
    Error { message: String },
    VfsListResponse { items: Vec<FileNode> },
    VfsReadFileResponse { content: String },
    Success,
    VfsListTrashResponse { items: Vec<TrashedFileNode> },
}

#[derive(Serialize, Debug)]
#[serde(untagged)]
pub enum ServerMessage {
    Response(ServerResponse),
    Push(ServerPush),
}

#[derive(Serialize, Debug)]
pub struct ServerPush {
    #[serde(flatten)]
    pub payload: ServerPushPayload,
}

#[derive(Serialize, Debug)]
#[serde(tag = "type", content = "payload")]
#[serde(rename_all = "camelCase")]
pub enum ServerPushPayload {
    TerminalOutput { output: String },
    VfsUpdate { path: String },
}

#[derive(Serialize, Debug, Clone)]
pub struct UserInfo {
    pub id: i64,
    pub username: String,
    pub role: String,
}

#[derive(Serialize, Debug, sqlx::FromRow)]
pub struct FileNode {
    pub name: String,
    pub node_type: String,
    pub size: i64,
    pub updated_at: DateTime<Utc>,
}

#[derive(Serialize, Debug, sqlx::FromRow)]
pub struct TrashedFileNode {
    pub id: i64,
    pub name: String,
    pub original_path: String,
    pub trashed_at: DateTime<Utc>,
}
EOF

cat > src/session.rs << 'EOF'
use axum::extract::ws::{Message, WebSocket};
use futures_util::{stream::{SplitSink}, StreamExt};
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::mpsc;
use crate::db::{self, DbPool};
use crate::pty_handler::{PtyHandler, PtyMessage};
use crate::protocol::{ClientRequest, ClientRequestPayload, ServerMessage, ServerPush, ServerPushPayload, ServerResponse, ServerResponsePayload, UserInfo};
use crate::vfs;

pub struct UserSession {
    ws: WebSocket,
    db_pool: Arc<DbPool>,
    pty_handler: PtyHandler,
    user: Option<UserInfo>,
    cwd: PathBuf,
}

impl UserSession {
    pub fn new(socket: WebSocket, db_pool: Arc<DbPool>) -> Self {
        Self {
            ws: socket,
            db_pool,
            pty_handler: PtyHandler::new(),
            user: None,
            cwd: PathBuf::from("/"),
        }
    }

    pub async fn run(mut self) {
        let (mut ws_sender, mut ws_receiver) = self.ws.split();
        let (pty_tx, mut pty_rx) = mpsc::unbounded_channel();
        
        loop {
            tokio::select! {
                ws_msg = ws_receiver.next() => {
                    if let Some(Ok(msg)) = ws_msg {
                        if self.handle_client_message(msg, &pty_tx, &mut ws_sender).await.is_err() { break; }
                    } else { break; }
                },
                pty_msg = pty_rx.recv() => {
                    if let Some(PtyMessage::Output(output)) = pty_msg {
                        let _ = self.send_push(ServerPushPayload::TerminalOutput { output }, &mut ws_sender).await;
                    } else { break; }
                }
            }
        }
        tracing::debug!("User session for '{:?}' ended.", self.user.as_ref().map(|u| &u.username));
    }

    async fn handle_client_message(&mut self, msg: Message, pty_tx: &mpsc::UnboundedSender<PtyMessage>, ws_sender: &mut SplitSink<WebSocket, Message>) -> Result<(), ()> {
        if let Message::Text(text) = msg {
            match serde_json::from_str::<ClientRequest>(&text) {
                Ok(req) => {
                    let req_id = req.request_id.clone();
                    if self.user.is_none() {
                        if let ClientRequestPayload::Login { username, password } = req.payload {
                            self.handle_login(req_id, username, password, pty_tx, ws_sender).await;
                        } else {
                            self.send_error_response(req_id, "Authentication required".to_string(), ws_sender).await;
                        }
                    } else {
                        self.handle_authenticated_request(req, ws_sender).await;
                    }
                }
                Err(e) => self.send_error_response("unknown".to_string(), format!("Invalid request format: {}", e), ws_sender).await,
            }
        } else if let Message::Close(_) = msg {
            return Err(());
        }
        Ok(())
    }
    
    async fn handle_login(&mut self, req_id: String, username: String, password: String, pty_tx: &mpsc::UnboundedSender<PtyMessage>, ws_sender: &mut SplitSink<WebSocket, Message>) {
        match db::verify_password(&self.db_pool, &username, &password).await {
            Ok(Some(user)) => {
                let home_dir = PathBuf::from(format!("/home/{}", &user.username));
                if self.pty_handler.spawn(home_dir.clone(), pty_tx.clone()).is_ok() {
                    self.cwd = home_dir;
                    self.user = Some(user.clone());
                    self.send_response(req_id, ServerResponsePayload::LoginSuccess { user }, ws_sender).await;
                } else {
                    self.send_error_response(req_id, "Failed to start terminal session".to_string(), ws_sender).await;
                }
            }
            Ok(None) => self.send_error_response(req_id, "Invalid credentials".to_string(), ws_sender).await,
            Err(e) => self.send_error_response(req_id, format!("Login error: {}", e), ws_sender).await,
        }
    }

    async fn handle_authenticated_request(&mut self, req: ClientRequest, ws_sender: &mut SplitSink<WebSocket, Message>) {
        let user_id = self.user.as_ref().unwrap().id;
        let req_id = req.request_id;
        let user_home_dir = format!("/home/{}", self.user.as_ref().unwrap().username);

        let resolve = |p: &str| vfs::resolve_path(&self.cwd, p, &user_home_dir).to_string_lossy().to_string();

        match req.payload {
            ClientRequestPayload::RunCommand { command } => {
                if command.trim().starts_with("cd ") {
                    let target = command.trim().split_whitespace().nth(1).unwrap_or("~");
                    self.cwd = vfs::resolve_path(&self.cwd, target, &user_home_dir);
                }
                self.pty_handler.send_command(command + "\n");
            }
            ClientRequestPayload::VfsList { path } => {
                match vfs::list_directory(&self.db_pool, user_id, &resolve(&path)).await {
                    Ok(items) => self.send_response(req_id, ServerResponsePayload::VfsListResponse { items }, ws_sender).await,
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsReadFile { path } => {
                match vfs::read_file_content(&self.db_pool, user_id, &resolve(&path)).await {
                    Ok(content) => self.send_response(req_id, ServerResponsePayload::VfsReadFileResponse { content }, ws_sender).await,
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsWriteFile { path, content } => {
                let resolved_path = resolve(&path);
                match vfs::write_file_content(&self.db_pool, user_id, &resolved_path, &content).await {
                    Ok(_) => { self.send_response_and_push_vfs(req_id, resolved_path, ws_sender).await; },
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsCreateNode { path, node_type } => {
                let resolved_path = resolve(&path);
                match vfs::create_node(&self.db_pool, user_id, &resolved_path, &node_type).await {
                    Ok(_) => { self.send_response_and_push_vfs(req_id, resolved_path, ws_sender).await; },
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsMoveNode { old_path, new_path } => {
                let resolved_old = resolve(&old_path);
                let resolved_new = resolve(&new_path);
                match vfs::move_node(&self.db_pool, user_id, &resolved_old, &resolved_new).await {
                    Ok(_) => {
                        self.send_response(req_id, ServerResponsePayload::Success, ws_sender).await;
                        let _ = self.send_push(ServerPushPayload::VfsUpdate{ path: resolved_old }, ws_sender).await;
                        let _ = self.send_push(ServerPushPayload::VfsUpdate{ path: resolved_new }, ws_sender).await;
                    },
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsTrashNode { path } => {
                let resolved_path = resolve(&path);
                match vfs::trash_node(&self.db_pool, user_id, &resolved_path).await {
                    Ok(_) => { self.send_response_and_push_vfs(req_id, resolved_path, ws_sender).await; },
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsListTrash => {
                match vfs::list_trash(&self.db_pool, user_id).await {
                    Ok(items) => self.send_response(req_id, ServerResponsePayload::VfsListTrashResponse { items }, ws_sender).await,
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsRestoreNode { id } => {
                match vfs::restore_node(&self.db_pool, user_id, id).await {
                    Ok(path) => { self.send_response_and_push_vfs(req_id, path, ws_sender).await; },
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsDeleteNode { id } => {
                match vfs::permanently_delete_node(&self.db_pool, user_id, id).await {
                    Ok(_) => self.send_response(req_id, ServerResponsePayload::Success, ws_sender).await,
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            ClientRequestPayload::VfsEmptyTrash => {
                match vfs::empty_trash(&self.db_pool, user_id).await {
                    Ok(_) => self.send_response(req_id, ServerResponsePayload::Success, ws_sender).await,
                    Err(e) => self.send_error_response(req_id, e.to_string(), ws_sender).await,
                }
            }
            _ => self.send_error_response(req_id, "Unsupported action".to_string(), ws_sender).await,
        }
    }
    
    async fn send_response_and_push_vfs(&self, req_id: String, path: String, ws_sender: &mut SplitSink<WebSocket, Message>) {
        self.send_response(req_id, ServerResponsePayload::Success, ws_sender).await;
        let _ = self.send_push(ServerPushPayload::VfsUpdate{ path }, ws_sender).await;
    }
    
    async fn send_response(&self, request_id: String, payload: ServerResponsePayload, sender: &mut SplitSink<WebSocket, Message>) {
        let response = ServerMessage::Response(ServerResponse { request_id, payload });
        if let Ok(json) = serde_json::to_string(&response) {
            if sender.send(Message::Text(json)).await.is_err() {
                tracing::warn!("Failed to send response to client.");
            }
        }
    }

    async fn send_error_response(&self, request_id: String, message: String, sender: &mut SplitSink<WebSocket, Message>) {
        tracing::error!("Sending error to client: {}", message);
        self.send_response(request_id, ServerResponsePayload::Error { message }, sender).await;
    }
    
    async fn send_push(&self, payload: ServerPushPayload, sender: &mut SplitSink<WebSocket, Message>) {
        let push = ServerMessage::Push(ServerPush { payload });
        if let Ok(json) = serde_json::to_string(&push) {
            if sender.send(Message::Text(json)).await.is_err() {
                tracing::warn!("Failed to send push notification to client.");
            }
        }
    }
}
EOF

cat > src/pty_handler.rs << 'EOF'
use pty_process_tokio::PtyProcess;
use std::path::PathBuf;
use std::process::Command;
use tokio::io::{AsyncWriteExt, AsyncReadExt};
use tokio::sync::mpsc;

pub enum PtyMessage {
    Output(String),
}

pub struct PtyHandler {
    pty_writer: Option<mpsc::UnboundedSender<String>>,
}

impl PtyHandler {
    pub fn new() -> Self { Self { pty_writer: None } }

    pub fn spawn(&mut self, _cwd: PathBuf, output_tx: mpsc::UnboundedSender<PtyMessage>) -> Result<(), String> {
        let process = PtyProcess::spawn(Command::new("bash")).map_err(|e| e.to_string())?;
        let (pty_tx, mut pty_rx) = mpsc::unbounded_channel::<String>();
        self.pty_writer = Some(pty_tx);

        let mut master = process.master.clone();
        let mut child_writer = process.child_writer.clone();

        tokio::spawn(async move {
            while let Some(cmd) = pty_rx.recv().await {
                if child_writer.write_all(cmd.as_bytes()).await.is_err() { break; }
            }
        });

        tokio::spawn(async move {
            let mut buf = [0u8; 4096];
            loop {
                match master.read(&mut buf).await {
                    Ok(0) | Err(_) => { break; }
                    Ok(n) => {
                        if let Ok(s) = String::from_utf8(buf[..n].to_vec()) {
                            if output_tx.send(PtyMessage::Output(s)).is_err() { break; }
                        }
                    }
                }
            }
        });

        Ok(())
    }

    pub fn send_command(&self, cmd: String) {
        if let Some(writer) = &self.pty_writer {
            if writer.send(cmd).is_err() {
                tracing::error!("Failed to send command to PTY writer task.");
            }
        }
    }
}
EOF

cat > src/vfs.rs << 'EOF'
use crate::db::DbPool;
use crate::protocol::{FileNode, TrashedFileNode};
use anyhow::{anyhow, Result};
use chrono::Utc;
use sqlx::Row;
use std::env;
use std::path::{Path, PathBuf};
use tokio::fs;
use uuid::Uuid;

pub async fn list_directory(pool: &DbPool, user_id: i64, path_str: &str) -> Result<Vec<FileNode>> {
    let parent_id = get_path_id(pool, user_id, Path::new(path_str)).await?;
    let query = "SELECT name, node_type, size, updated_at FROM files WHERE owner_id = ? AND parent_id IS ? AND is_trashed = FALSE ORDER BY node_type DESC, name ASC";
    let items = sqlx::query_as(query)
        .bind(user_id)
        .bind(parent_id)
        .fetch_all(pool)
        .await?;
    Ok(items)
}

pub async fn read_file_content(pool: &DbPool, user_id: i64, path_str: &str) -> Result<String> {
    let (disk_path_str,): (String,) =
        sqlx::query_as("SELECT disk_path FROM files WHERE id = ? AND owner_id = ? AND node_type = 'file'")
            .bind(get_path_id(pool, user_id, Path::new(path_str)).await?.ok_or_else(|| anyhow!("File not found"))?)
            .bind(user_id)
            .fetch_one(pool)
            .await?;
    
    let content = fs::read(disk_path_str).await?;
    Ok(base64::encode(content))
}

pub async fn write_file_content(pool: &DbPool, user_id: i64, path_str: &str, base64_content: &str) -> Result<()> {
    let file_id = get_path_id(pool, user_id, Path::new(path_str)).await?.ok_or_else(|| anyhow!("File not found"))?;
    let content = base64::decode(base64_content)?;

    let (disk_path_str,): (Option<String>,) = sqlx::query_as("SELECT disk_path FROM files WHERE id = ?")
        .bind(file_id)
        .fetch_one(pool)
        .await?;
    
    if let Some(disk_path) = disk_path_str {
        fs::write(disk_path, &content).await?;
        sqlx::query("UPDATE files SET size = ?, updated_at = ? WHERE id = ?")
            .bind(content.len() as i64)
            .bind(Utc::now())
            .bind(file_id)
            .execute(pool)
            .await?;
        Ok(())
    } else {
        Err(anyhow!("Node is a directory, not a file"))
    }
}

pub async fn create_node(pool: &DbPool, user_id: i64, path_str: &str, node_type: &str) -> Result<()> {
    let path = Path::new(path_str);
    let name = path.file_name().and_then(|s| s.to_str()).ok_or_else(|| anyhow!("Invalid path or name"))?;
    let parent_path = path.parent().unwrap_or(Path::new("/"));
    let parent_id = get_path_id(pool, user_id, parent_path).await?;

    let mut tx = pool.begin().await?;

    let disk_path = if node_type == "file" {
        let storage_root = env::var("STORAGE_ROOT").unwrap_or_else(|_| "/tmp/cde_storage".to_string());
        fs::create_dir_all(&storage_root).await?;
        let disk_filename = Uuid::new_v4().to_string();
        let path = Path::new(&storage_root).join(disk_filename);
        fs::write(&path, "").await?;
        Some(path.to_str().unwrap().to_string())
    } else {
        None
    };

    sqlx::query("INSERT INTO files (owner_id, parent_id, name, node_type, disk_path, original_path) VALUES (?, ?, ?, ?, ?, ?)")
        .bind(user_id)
        .bind(parent_id)
        .bind(name)
        .bind(node_type)
        .bind(disk_path)
        .bind(path_str)
        .execute(&mut *tx)
        .await?;
    
    tx.commit().await?;
    Ok(())
}

pub async fn trash_node(pool: &DbPool, user_id: i64, path_str: &str) -> Result<()> {
    let node_id = get_path_id(pool, user_id, Path::new(path_str)).await?.ok_or_else(|| anyhow!("Node not found"))?;
    sqlx::query("UPDATE files SET is_trashed = TRUE, trashed_at = ? WHERE id = ? AND owner_id = ?")
        .bind(Utc::now())
        .bind(node_id)
        .bind(user_id)
        .execute(pool)
        .await?;
    Ok(())
}

pub async fn list_trash(pool: &DbPool, user_id: i64) -> Result<Vec<TrashedFileNode>> {
    let items = sqlx::query_as(
        "SELECT id, name, original_path, trashed_at FROM files WHERE owner_id = ? AND is_trashed = TRUE ORDER BY trashed_at DESC"
    )
    .bind(user_id)
    .fetch_all(pool)
    .await?;
    Ok(items)
}

pub async fn restore_node(pool: &DbPool, user_id: i64, node_id: i64) -> Result<String> {
    let (original_path,): (String,) = sqlx::query_as("SELECT original_path FROM files WHERE id = ? AND owner_id = ?")
        .bind(node_id)
        .bind(user_id)
        .fetch_one(pool)
        .await?;
    
    sqlx::query("UPDATE files SET is_trashed = FALSE, trashed_at = NULL WHERE id = ?")
        .bind(node_id)
        .execute(pool)
        .await?;

    Ok(original_path)
}

pub async fn permanently_delete_node(pool: &DbPool, user_id: i64, node_id: i64) -> Result<()> {
    let row = sqlx::query("SELECT disk_path FROM files WHERE id = ? AND owner_id = ? AND is_trashed = TRUE")
        .bind(node_id)
        .bind(user_id)
        .fetch_optional(pool)
        .await?;
    
    if let Some(row) = row {
        if let Ok(Some(disk_path)) = row.try_get::<Option<String>, _>("disk_path") {
            let _ = fs::remove_file(disk_path).await;
        }
        sqlx::query("DELETE FROM files WHERE id = ?").bind(node_id).execute(pool).await?;
    }
    Ok(())
}

pub async fn empty_trash(pool: &DbPool, user_id: i64) -> Result<()> {
     let trashed_files = sqlx::query_as::<_, (i64, Option<String>)>("SELECT id, disk_path FROM files WHERE owner_id = ? AND is_trashed = TRUE")
        .bind(user_id)
        .fetch_all(pool)
        .await?;
    
    let mut tx = pool.begin().await?;
    for (id, disk_path) in trashed_files {
        if let Some(path) = disk_path {
            let _ = fs::remove_file(path).await;
        }
        sqlx::query("DELETE FROM files WHERE id = ?").bind(id).execute(&mut *tx).await?;
    }
    tx.commit().await?;
    Ok(())
}

pub async fn move_node(pool: &DbPool, user_id: i64, old_path_str: &str, new_path_str: &str) -> Result<()> {
    let old_path = Path::new(old_path_str);
    let new_path = Path::new(new_path_str);

    let node_id = get_path_id(pool, user_id, old_path).await?.ok_or_else(|| anyhow!("Source not found"))?;
    
    let new_parent_path = new_path.parent().unwrap_or(Path::new("/"));
    let new_name = new_path.file_name().and_then(|s| s.to_str()).ok_or_else(|| anyhow!("Invalid new path"))?;
    let new_parent_id = get_path_id(pool, user_id, new_parent_path).await?;

    sqlx::query("UPDATE files SET parent_id = ?, name = ?, original_path = ?, updated_at = ? WHERE id = ? AND owner_id = ?")
        .bind(new_parent_id)
        .bind(new_name)
        .bind(new_path_str)
        .bind(Utc::now())
        .bind(node_id)
        .bind(user_id)
        .execute(pool)
        .await?;
        
    Ok(())
}

async fn get_path_id(pool: &DbPool, user_id: i64, path: &Path) -> Result<Option<i64>> {
    let components: Vec<&str> = path.to_str().unwrap_or("").split('/').filter(|&s| !s.is_empty()).collect();
    let mut current_id: Option<i64> = None;
    for component in components {
        let result: Option<(i64,)> = sqlx::query_as(
            "SELECT id FROM files WHERE owner_id = ? AND parent_id IS ? AND name = ? AND is_trashed = FALSE",
        )
        .bind(user_id)
        .bind(current_id)
        .bind(component)
        .fetch_optional(pool)
        .await?;
        
        current_id = result.map(|(id,)| id);
        if current_id.is_none() { return Ok(None); }
    }
    Ok(current_id)
}

pub fn resolve_path(cwd: &Path, target: &str, home: &str) -> PathBuf {
    let mut new_path = if target.starts_with('/') {
        PathBuf::from(target)
    } else if target == "~" {
        PathBuf::from(home)
    } else if target.starts_with("~/") {
        PathBuf::from(home).join(&target[2..])
    } else {
        cwd.join(target)
    };

    let mut components = Vec::new();
    for component in new_path.components() {
        match component {
            std::path::Component::Normal(name) => components.push(name.to_str().unwrap()),
            std::path::Component::ParentDir => { components.pop(); },
            _ => {}
        }
    }
    
    let mut result = PathBuf::from("/");
    for component in components {
        result.push(component);
    }
    result
}
EOF

# Create migrations directory and files
echo "   - Writing database migrations..."
mkdir -p migrations

cat > migrations/20240725000001_initial_schema.sql << 'EOF'
CREATE TABLE IF NOT EXISTS users (
    id INTEGER PRIMARY KEY,
    username TEXT NOT NULL UNIQUE,
    password_hash TEXT NOT NULL,
    role TEXT NOT NULL CHECK(role IN ('Admin', 'Standard', 'Limited')),
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS files (
    id INTEGER PRIMARY KEY,
    owner_id INTEGER NOT NULL,
    parent_id INTEGER,
    name TEXT NOT NULL,
    node_type TEXT NOT NULL CHECK(node_type IN ('dir', 'file')),
    disk_path TEXT UNIQUE,
    size INTEGER NOT NULL DEFAULT 0,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    original_path TEXT NOT NULL,
    FOREIGN KEY (owner_id) REFERENCES users (id) ON DELETE CASCADE,
    FOREIGN KEY (parent_id) REFERENCES files (id) ON DELETE CASCADE,
    UNIQUE (owner_id, parent_id, name)
);

CREATE INDEX IF NOT EXISTS idx_files_parent ON files (parent_id);
CREATE INDEX IF NOT EXISTS idx_files_owner ON files (owner_id);
EOF

cat > migrations/20240726000001_add_trash.sql << 'EOF'
ALTER TABLE files ADD COLUMN is_trashed BOOLEAN NOT NULL DEFAULT FALSE;
ALTER TABLE files ADD COLUMN trashed_at TIMESTAMP;
CREATE INDEX IF NOT EXISTS idx_files_trashed ON files (owner_id, is_trashed);
EOF

# --- Database Setup and Compilation ---
echo "   - Setting up database..."
# Read DATABASE_URL from .env and export it for sqlx
db_url=$(grep -E '^DATABASE_URL=' .env | cut -d '=' -f2- | tr -d '\r\n')
if [ -z "$db_url" ]; then
    echo -e "${RED}Error: DATABASE_URL not found in .env${NC}"
    exit 1
fi
export DATABASE_URL="$db_url"

# If using sqlite:..., ensure parent directory for DB file exists
if [[ "$db_url" == sqlite:* ]]; then
    db_path="${db_url#sqlite:}"
    db_dir=$(dirname "$db_path")
    if [[ "$db_dir" != "." && -n "$db_dir" ]]; then
        mkdir -p "$db_dir"
    fi
fi

# Create database and run migrations
sqlx database create
echo "   - Running database migrations..."
sqlx migrate run

echo "   - Compiling backend (this may take a moment on first run)..."
# Build the release binary (manifest is the current Cargo.toml)
cargo build --release --manifest-path ./Cargo.toml

# Return to root directory
cd ..
echo "   - Switched to directory: $(pwd)"

echo -e "\n3. Setting up Frontend..."
# Install frontend dev server
echo "   - Installing @web/dev-server..."
npm install @web/dev-server > /dev/null 2>&1

echo -e "\n4. Launching Servers..."
# Start Backend
echo "   - Starting backend server..."
nohup ./"$BACKEND_DIR"/target/release/"$BACKEND_DIR" > "$BACKEND_LOG_FILE" 2>&1 &
BACKEND_PID=$!
echo $BACKEND_PID > "$BACKEND_PID_FILE"
echo "     - Backend running in background (PID: $BACKEND_PID). Log: $BACKEND_LOG_FILE"

# Start Frontend
echo "   - Starting frontend live-reload server..."
nohup npx wds --node-resolve --open --watch --port $FRONTEND_PORT > "$FRONTEND_LOG_FILE" 2>&1 &
FRONTEND_PID=$!
echo $FRONTEND_PID > "$FRONTEND_PID_FILE"
echo "     - Frontend server running in background (PID: $FRONTEND_PID). Log: $FRONTEND_LOG_FILE"

echo -e "\n${GREEN}--- Setup Complete! ---${NC}"
echo -e "Application starting in your browser at http://localhost:$FRONTEND_PORT"
echo "To stop the servers, run: ./stop.sh"